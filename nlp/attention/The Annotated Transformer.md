

```python

```


```python
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math, copy, time
from torch.autograd import Variable
import matplotlib.pyplot as plt
import seaborn
seaborn.set_context(context="talk")
%matplotlib inline
```

## æ¨¡å‹æ¶æ„
å¤§å¤šæ•°competitive neural sequence transduction modelséƒ½æœ‰encoder-decoderæ¶æ„([å‚è€ƒè®ºæ–‡](https://arxiv.org/abs/1409.0473))ã€‚æœ¬æ–‡ä¸­ï¼Œencoderå°†ç¬¦å·è¡¨ç¤ºçš„è¾“å…¥åºåˆ—$x_1, \dots, x_n$æ˜ å°„åˆ°ä¸€ç³»åˆ—è¿ç»­è¡¨ç¤º$Z=(z_1, \dots, z_n)$ã€‚ç»™å®šä¸€ä¸ªzï¼Œdecoderä¸€æ¬¡äº§ç”Ÿä¸€ä¸ªç¬¦å·è¡¨ç¤ºçš„åºåˆ—è¾“å‡º$(y_1, \dots, y_m)$ã€‚å¯¹äºæ¯ä¸€æ­¥æ¥è¯´ï¼Œæ¨¡å‹éƒ½æ˜¯è‡ªå›å½’çš„([è‡ªå›å½’ä»‹ç»è®ºæ–‡](https://arxiv.org/abs/1308.0850)),åœ¨ç”Ÿæˆä¸‹ä¸€ä¸ªæ—¶æ¶ˆè€—å…ˆå‰ç”Ÿæˆçš„æ‰€æœ‰ç¬¦å·ä½œä¸ºé™„åŠ è¾“å…¥ã€‚


```python
class EncoderDecoder(nn.Module):
    """
    A stanard Encoder-Decoder architecture.Base fro this and many other models.
    """
    
    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.generator = generator
        
    def forward(self, src, tgt, src_mask, tgt_mask):
        """ Take in and process masked src and target sequences. """
        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)
    
    def encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)
    
    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
```


```python
class Generator(nn.Module):
    """Define standard linear + softmax generation step."""
    def __init__(self, d_model, vocab):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, vocab)
        
    def forward(self, x):
        return F.log_softmax(self.proj(x), dim=-1)
```

Transformerè¿™ç§ç»“æ„ï¼Œåœ¨encoderå’Œdecoderä¸­ä½¿ç”¨å †å çš„self-attentionå’Œpoint-wiseå…¨è¿æ¥å±‚ã€‚å¦‚ä¸‹å›¾çš„å·¦è¾¹å’Œå³è¾¹æ‰€ç¤ºï¼š


```python
Image(filename='images/ModelNet-21.png')
```




![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_6_0.png)



## Encoder å’Œ Decoder Stacks
### Encoder
ç¼–ç å™¨ç”±6ä¸ªç›¸åŒçš„layerå †å è€Œæˆ


```python
def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])
```


```python
class Encoder(nn.Module):
    "Core encoder is a stack of N layers"
    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)
        
    def forward(self, x, mask):
        "Pass the input (and mask) through each layer in turn."
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
```

è¿™é‡Œåœ¨ä¸¤ä¸ªå­å±‚ä¸­éƒ½ä½¿ç”¨äº†æ®‹å·®è¿æ¥([å‚è€ƒè®ºæ–‡](https://arxiv.org/abs/1512.03385))ï¼Œç„¶åç´§è·Ÿlayer normalization([å‚è€ƒè®ºæ–‡](https://arxiv.org/abs/1607.06450))


```python
class LayerNorm(nn.Module):
    """ Construct a layernorm model (See citation for details)"""
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps
        
    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
```

ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸ªå­å±‚çš„è¾“å‡ºæ˜¯LayerNorm(x + Sublayer(x))ï¼Œå…¶ä¸­Sublayer(x)ç”±å­å±‚å®ç°ã€‚å¯¹äºæ¯ä¸€ä¸ªå­å±‚ï¼Œå°†å…¶æ·»åŠ åˆ°å­å±‚è¾“å…¥å¹¶è¿›è¡Œè§„èŒƒåŒ–ä¹‹å‰ï¼Œä½¿ç”¨äº†Dropout([å‚è€ƒè®ºæ–‡](http://jmlr.org/papers/v15/srivastava14a.html))

ä¸ºäº†æ–¹ä¾¿æ®‹å·®è¿æ¥ï¼Œæ¨¡å‹ä¸­çš„æ‰€æœ‰å­å±‚å’Œembeddingå±‚è¾“å‡ºç»´åº¦éƒ½æ˜¯512


```python
class SublayerConnection(nn.Module):
    """ 
    A residual connection followed by a layer norm. Note for 
    code simplicity the norm is first as opposed to last .
    """
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, sublayer):
        """Apply residual connection to any sublayer with the sanme size. """
        return x + self.dropout(sublayer(self.norm(x)))
```

æ¯å±‚æœ‰ä¸¤ä¸ªå­å±‚ã€‚ç¬¬ä¸€ä¸ªå­å±‚æ˜¯multi-head self-attentionæœºåˆ¶ï¼Œç¬¬äºŒå±‚æ˜¯ä¸€ä¸ªç®€å•çš„position-wiseå…¨è¿æ¥å‰é¦ˆç¥ç»ç½‘ç»œã€‚


```python
class EncoderLayer(nn.Module):
    """Encoder is made up of self-attention and feed forward (defined below)"""
    def __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 2)
        self.size = size
    
    def forward(self, x, mask):
        """Follow Figure 1 (left) for connection """
        x = self.sublayer[0](x, lambda x : self.self_attn(x, x, x, mask))
        return self.sublayer[1](x, self.feed_forward)
```

### Decoder
Decoderç”±6ä¸ªç›¸åŒlayerå †æˆ


```python
class Decoder(nn.Module):
    """Generic N layer decoder with masking """
    def __init__(self, layer, N):
        super(Decoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)
        
    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)
```

æ¯ä¸ªencoderå±‚é™¤äº†ä¸¤ä¸ªå­å±‚å¤–ï¼Œè¿˜æ’å…¥äº†ç¬¬ä¸‰ä¸ªå­å±‚ï¼Œå³åœ¨encoderå †çš„è¾“å‡ºä¸Šä¸Šæ‰§è¡Œmulti-headæ³¨æ„åŠ›ä½œç”¨çš„å±‚ã€‚ç±»ä¼¼äºencoderï¼Œåœ¨æ¯ä¸€ä¸ªå­å±‚åé¢ä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œå¹¶ç´§è·Ÿnorm


```python
class DecoderLayer(nn.Module):
    """Decoder is made of self-attn, src-attn, and feed forward (defined below)"""
    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 3)
        
    def forward(self, x, memory, src_mask, tgt_mask):
        """Follow Figure 1 (right) for connections"""
        m = memory
        x = self.sublayer[0](x, lambda x : self.self_attn(x, x, x, tgt_mask))
        x = self.sublayer[1](x, lambda x : self.src_attn(x, m, m, src_mask))
        return self.sublayer[2](x, self.feed_forward)
```

ä¿®æ”¹åœ¨decoderå±‚å †ä¸­çš„self-atention å­å±‚ï¼Œé˜²æ­¢ä½ç½®å…³æ³¨åç»­ä½ç½®ã€‚maskingä¸ä½¿ç”¨ä¸€ä¸ªpositionä¿¡æ¯åç§»çš„è¾“å‡ºembeddingç›¸ç»“åˆï¼Œç¡®ä¿å¯¹äºposition $i$ çš„é¢„æµ‹ä»…ä¾èµ–äºå°äº $i$ çš„positionçš„è¾“å‡º


```python
def subsequent_mask(size):
    """Mask out subsequent positions. """
    attn_shape = (1, size, size)
    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')
    return torch.from_numpy(subsequent_mask) == 0
```


```python
plt.figure(figsize=(5, 5))
plt.imshow(subsequent_mask(20)[0])
None
```


![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_22_0.png)


## Attention
æ³¨æ„åŠ›åŠŸèƒ½å¯ä»¥çœ‹åšå°†ä¸€ä¸ªqueryå’Œä¸€ç»„key-valueå¯¹æ˜ å°„åˆ°ä¸€ä¸ªoutputï¼Œå…¶ä¸­queryã€keysã€valueså’Œoutputéƒ½æ˜¯å‘é‡(vector)ï¼Œè¾“å‡ºæ˜¯valuesçš„åŠ æƒå’Œï¼Œå…¶ä¸­æƒé‡å¯ä»¥é€šè¿‡å°†queryå’Œå¯¹åº”çš„keyè¾“å…¥åˆ°ä¸€ä¸ªcompatibility functionæ¥è®¡ç®—åˆ†é…ç»™æ¯ä¸€ä¸ªvalueçš„æƒé‡ã€‚

è¿™é‡Œçš„attentionå…¶å®å¯ä»¥å«åšâ€œScaled Dot-Product Attentionâ€ã€‚è¾“å…¥ç”±$d_k$ç»´åº¦çš„querieså’Œkeysç»„æˆï¼Œvaluesçš„ç»´åº¦æ˜¯$d_v$ã€‚è®¡ç®—queryå’Œæ‰€æœ‰keysçš„ç‚¹ä¹˜ï¼Œç„¶åé™¤ä»¥$\sqrt{d_k}$ï¼Œç„¶ååº”ç”¨softmaxå‡½æ•°æ¥è·å–å€¼çš„æƒé‡ã€‚$\sqrt{d_k}$èµ·åˆ°è°ƒèŠ‚ä½œç”¨ï¼Œä½¿å¾—å†…ç§¯ä¸è‡³äºå¤ªå¤§ï¼ˆå¤ªå¤§çš„è¯softmaxåå°±é0å³1äº†ï¼Œä¸å¤Ÿâ€œsoftâ€äº†ï¼‰ã€‚

å®é™…è®¡ç®—ä¸­ï¼Œä¸€æ¬¡è®¡ç®—ä¸€ç»„queriesçš„æ³¨æ„åŠ›å‡½æ•°ï¼Œå°†å…¶ç»„æˆä¸€ä¸ªçŸ©é˜µ$Q$, å¹¶ä¸”keyså’Œvaluesä¹Ÿåˆ†åˆ«ç»„æˆçŸ©é˜µ$K$å’Œ$V$ã€‚æ­¤æ—¶ï¼Œä½¿ç”¨å¦‚ä¸‹å…¬å¼è¿›è¡Œè®¡ç®—ï¼š
$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$


```python
def attention(query, key, value, mask=None, dropout=None):
    """Compute 'Scaled Dot Product Attention ' """
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # matmulçŸ©é˜µç›¸ä¹˜
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = F.softmax(scores, dim = -1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn
```

æœ€å¸¸ç”¨çš„ä¸¤ç§æ³¨æ„åŠ›å®ç°æœºåˆ¶åŒ…æ‹¬ï¼š additive attention (cite), and dot-product (multiplicative) attention.
æ­¤å¤„çš„å®ç°æ˜¯dot-product attentionï¼Œä¸è¿‡å¤šäº†$\sqrt{d_k}$ã€‚additive attentionè®¡ç®—å‡½æ•°ä½¿ç”¨ä¸€ä¸ªä½†éšè—å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œã€‚
è¿™ä¸¤ç§å®ç°æœºåˆ¶åœ¨ç†è®ºä¸Šå¤æ‚åº¦æ˜¯ç›¸ä¼¼çš„ï¼Œä½†æ˜¯dot-product attentioné€Ÿåº¦æ›´å¿«å’Œæ›´èŠ‚çœç©ºé—´ï¼Œå› ä¸ºå¯ä»¥ä½¿ç”¨é«˜åº¦ä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•æ¥å®ç°ã€‚

å¯¹äºå°è§„æ¨¡valuesä¸¤ç§æœºåˆ¶æ€§èƒ½ç±»å·®ä¸å¤šï¼Œä½†æ˜¯å¯¹äºå¤§è§„æ¨¡çš„valuesä¸Šï¼Œadditive attention æ€§èƒ½ä¼˜äº dot poductã€‚
åŸå› åˆ†æï¼šçŒœæµ‹å¯èƒ½æ˜¯å¯¹äºå¤§è§„æ¨¡valuesï¼Œå†…ç§¯ä¼šå·¨å¹…å¢é•¿ï¼Œå°†softmaxå‡½æ•°æ¨å…¥æœ‰ä¸€ä¸ªæå°æ¢¯åº¦çš„åŒºåŸŸï¼Œé€ æˆæ€§èƒ½ä¸‹é™ï¼ˆä¸ºäº†è¯´æ˜ä¸ºä»€ä¹ˆå†…ç§¯å˜å¤§ï¼Œå‡è®¾$qå’Œk$ æ˜¯ç‹¬ç«‹ä¸”å¹³å‡å€¼ä¸º0æ–¹å·®ä¸º1çš„éšæœºå˜é‡ï¼Œé‚£ä¹ˆç‚¹ä¹˜$q*k = \sum^{d_k}_{i=1}q_ik_i$ï¼Œå…¶å¹³å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼‰ä¸ºäº†æŠµæ¶ˆè´Ÿé¢å½±å“ï¼Œä½¿ç”¨$\sqrt{d_k}$æ¥ç¼©æ”¾å†…ç§¯


```python
from IPython.display import Image
Image("images/ModalNet-20.png")
```




![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_26_0.png)



Multi-head attentionå…è®¸æ¨¡å‹å…±åŒå…³æ³¨åœ¨ä¸åŒä½ç½®çš„æ¥è‡ªä¸åŒå­ç©ºé—´çš„è¡¨ç¤ºä¿¡æ¯ï¼Œåªè¦ä¸€ä¸ªå•ç‹¬çš„attention headï¼Œå¹³å‡ä¸€ä¸‹å°±ä¼šæŠ‘åˆ¶ä¸Šé¢æ‰€è¯´çš„æƒ…å†µã€‚æ­¤æ—¶ï¼Œç”¨å…¬å¼è¡¨ç¤ºå¦‚ä¸‹ï¼š
$MultiHead(Q, K, V) = Concat(head-1, \dots, head_h)W^o$ 

å…¶ä¸­$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$ï¼Œ$W_i^Q \in \mathcal{R}^{d_model * D_k}, W_i^K \in \mathcal{d_model * d_k}, W_i^V \in \mathcal{d_model*d_v} å¹¶ä¸” W_o \ in \mathcal{R}^{hd_v*d_{model}}$

æ­¤å¤„ï¼Œä½¿ç”¨h=8å¹³è¡Œçš„attentionå±‚æˆ–è€…headsï¼Œå¯¹æ¯ä¸€å±‚ä½¿ç”¨$d_k=d_v=d_{model}/h=64$


```python
class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        """ Take in model size and numbe of heads """
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)
        
    def forward(self, query, key, value, mask=None):
        """å›¾ç‰‡ModalNet-20çš„å®ç°"""
        if mask is not None:
            # åŒæ ·çš„maskåº”ç”¨åˆ°æ‰€æœ‰heads
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)
        
        # 1. æ‰¹é‡åšlinearæŠ•å½± => h x d_k
        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) 
                            for l, x in zip(self.linears, (query, key, value))]
        # 2. æ‰¹é‡åº”ç”¨attentionæœºåˆ¶åœ¨æ‰€æœ‰çš„æŠ•å½±å‘é‡ä¸Š
        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)
        
        # 3. ä½¿ç”¨viewè¿›è¡Œâ€œConcatâ€å¹¶ä¸”è¿›è¡Œæœ€åä¸€å±‚çš„linear
        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)
```

### æ¨¡å‹ä¸­attentionçš„åº”ç”¨
å¯¹äºTransformeræ¨¡å‹æ¥è¯´ï¼Œä½¿ç”¨ä¸‰ç§ä¸åŒçš„æ–¹å¼æ¥ä½¿ç”¨multi-head attention
* åœ¨â€œencoder-decoder attentionâ€å±‚ï¼Œqueriesæ¥è‡ªå‰ä¸€ä¸ªdecoderå±‚ï¼Œå¹¶ä¸”keyså’Œvaluesæ¥è‡ªencoderçš„è¾“å‡ºå±‚ã€‚è¿™ä½¿å¾—åœ¨decoderå±‚é‡Œçš„æ¯ä¸€ä¸ªä½ç½®ä¿¡æ¯å¯¹é½è¾“å…¥åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ï¼Œè¿™æ¨¡æ‹Ÿäº†sequence-to-sequenceæ¨¡å‹ä¸­çš„å…¸å‹çš„encoder-decoderæ³¨æ„åŠ›æœºåˆ¶
* encoderå±‚ä¸­åŒ…å«äº†self-attentionå±‚ã€‚åœ¨self-attentionå±‚ä¸­ï¼Œæ‰€æœ‰çš„keysã€valueså’Œquerieséƒ½æ¥è‡ªåŒä¸€å¤„ï¼Œåœ¨æ­¤ä¾‹ä¸­ï¼Œæ¥è‡ªencoderçš„å‰ä¸€å±‚ã€‚åœ¨encoderä¸­çš„æ¯ä¸€ä¸ªpositionå¯¹é½encoderçš„å‰ä¸€å±‚çš„æ‰€æœ‰position
* decoderå±‚ä¸­çš„self-attentionå…è®¸æ¯ä¸€ä¸ªpositionä½¿ç”¨decoderçš„åŒ…æ‹¬è¿™ä¸ªpositionåœ¨å†…çš„æ‰€æœ‰positionã€‚æˆ‘ä»¬éœ€è¦é˜²æ­¢ä¿¡æ¯æµå·¦æµæ¥ä¿è¯å…¶è‡ªå›å½’æ€§ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ç¼©æ”¾ç‚¹ä¹˜ attentionä¸­ä½¿ç”¨maskæŠ€æœ¯ï¼ˆè®¾ç½®ä¸ºè´Ÿæ— ç©·ï¼‰åº”ç”¨æ‰€æœ‰valuesï¼Œè¿™ä¸ªvaluesæ˜¯softmaxå±‚çš„è¾“å…¥ï¼Œå…¶å¯¹åº”éæ³•è¿æ¥ï¼ˆillegal connectionsï¼‰

## position-wiseå‰é¦ˆç¥ç»ç½‘ç»œ
é™¤äº†å­å±‚ä¸­çš„attentionï¼Œåœ¨encoderå’Œdecoderçš„æ‰€æœ‰å±‚ä¸­éƒ½åŒ…å«ä¸€ä¸ªå…¨è¿æ¥å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå®ƒå°†åˆ†åˆ«å’Œå…±åŒåº”ç”¨äºpositionã€‚å…¶ä¸­åŒ…æ‹¬ä¸¤ä¸ªä¸¤ä¸ªå¸¦æœ‰ReLUæ¿€æ´»å‡½æ•°çš„çº¿æ€§å˜æ¢ï¼š
$$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$
å¯¹äºä¸åŒçš„positionï¼Œä½¿ç”¨çš„çº¿æ€§å˜æ¢è™½ç„¶æ˜¯ç›¸åŒçš„ï¼Œä½†å±‚ä¸å±‚ä¹‹é—´çš„å‚æ•°æ˜¯ä¸åŒçš„ã€‚è¿™å…¶å®å°±æ˜¯ä¸¤ä¸ªå¤§å°ä¸º1çš„ä¸€ç»´å·ç§¯ã€‚è¾“å…¥å’Œè¾“å‡ºç»´åº¦éƒ½æ˜¯512ï¼Œå†…å±‚ç»´åº¦æ˜¯2048


```python
class PositionwiseFeedForward(nn.Module):
    """ 
    FFNå®ç° 
    d_model = 512
    d_ff = 2048
    """
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))
```

## Embeddingså’ŒSoftmax
è¿™é‡Œçš„Embeddingså’Œä¼ ç»Ÿçš„åºåˆ—ä»»åŠ¡ä¸€æ ·ï¼Œä½¿ç”¨è®­ç»ƒå¥½çš„Embeddingï¼Œå°†è¾“å…¥tokenå’Œè¾“å‡ºtokenå˜æˆè¯å‘é‡ï¼Œç»´åº¦ä¸ºd_modelã€‚
ä½¿ç”¨å¸¸è§çš„çº¿æ€§å˜æ¢å’Œsoftmaxå‡½æ•°å°†decoderçš„è¾“å‡ºå˜ä¸ºnext-tokençš„é¢„æµ‹æ¦‚ç‡ã€‚
åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œä¸¤ä¸ªembeddingå±‚å’Œpre-softmaxå±‚å…±äº«ç›¸åŒçš„æƒé‡çŸ©é˜µã€‚
åœ¨Embeddingå±‚ï¼Œå°†$\sqrt{d_{model}}$ä¹˜ä»¥è¿™äº›æƒé‡ã€‚


```python
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model
    
    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)
```

## Positional Encoding
å› ä¸ºåœ¨æ­¤æ¨¡å‹ä¸­ä¸åŒ…å«å·ç§¯å’Œå¾ªç¯ï¼ˆå±‚ï¼‰ï¼Œè¿™æ ·çš„æ¨¡å‹å¹¶ä¸èƒ½æ•æ‰åºåˆ—çš„é¡ºåºï¼æ¢å¥è¯è¯´ï¼Œå¦‚æœå°†K,VæŒ‰è¡Œæ‰“ä¹±é¡ºåºï¼ˆç›¸å½“äºå¥å­ä¸­çš„è¯åºæ‰“ä¹±ï¼‰ï¼Œé‚£ä¹ˆAttentionçš„ç»“æœè¿˜æ˜¯ä¸€æ ·çš„ã€‚å¯¹äºæ—¶é—´åºåˆ—æ¥è¯´ï¼Œå°¤å…¶æ˜¯å¯¹äºNLPä¸­çš„ä»»åŠ¡æ¥è¯´ï¼Œé¡ºåºæ˜¯å¾ˆé‡è¦çš„ä¿¡æ¯ï¼Œå®ƒä»£è¡¨ç€å±€éƒ¨ç”šè‡³æ˜¯å…¨å±€çš„ç»“æ„ï¼Œå­¦ä¹ ä¸åˆ°é¡ºåºä¿¡æ¯ï¼Œé‚£ä¹ˆæ•ˆæœå°†ä¼šå¤§æ‰“æŠ˜æ‰£ï¼ˆæ¯”å¦‚æœºå™¨ç¿»è¯‘ä¸­ï¼Œæœ‰å¯èƒ½åªæŠŠæ¯ä¸ªè¯éƒ½ç¿»è¯‘å‡ºæ¥äº†ï¼Œä½†æ˜¯ä¸èƒ½ç»„ç»‡æˆåˆç†çš„å¥å­ï¼‰ã€‚ä¸ºäº†ä½¿æ¨¡å‹èƒ½å¤Ÿå……åˆ†åˆ©ç”¨åºåˆ—ä¿¡æ¯ï¼Œå¿…é¡»ä¸ºæ¨¡å‹æ³¨å…¥tokensçš„ç›¸å¯¹æˆ–ç»å¯¹ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼ŒåŠ å…¥â€œposition embeddingâ€åˆ°encoderå’Œdecoderå±‚åº•éƒ¨çš„è¾“å…¥å±‚ä¸­ã€‚
position embeddingå’Œembeddingsæœ‰ç›¸åŒçš„ç»´åº¦ï¼Œéƒ½æ˜¯d_modelã€‚æ‰€ä»¥è¿™ä¸¤ä¸ªemebddingå¯ä»¥æ±‚å’Œã€‚
å¯¹äºposition embeddingï¼Œæœ‰å¾ˆå¤šé€‰æ‹©ï¼Œå¯ä»¥å‚è€ƒè¿™ç¯‡è®ºæ–‡ã€‚[ç‚¹å‡»æŸ¥çœ‹](https://arxiv.org/pdf/1705.03122.pdf)
åœ¨æ­¤æ¨¡å‹ä¸­ï¼Œä½¿ç”¨ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ï¼š
$$PE_{(pos, 2i)}(pï¼‰ = sin(pos/10000^{2i/d_{model}})ï¼ŒPE_{(pos, 2i+1)}(p)= cos(pos/10000^{2i/d_{modle}})$$
posä»£è¡¨positionï¼Œiä»£è¡¨ç»´åº¦ã€‚æ‰€ä»¥ï¼Œpositionç¼–ç çš„æ¯ä¸ªç»´åº¦å¯¹åº”æ­£å¼¦æ›²çº¿ã€‚è¿™é‡Œçš„æ„æ€æ˜¯å°†idä¸ºpçš„ä½ç½®æ˜ å°„ä¸ºä¸€ä¸ªdposç»´çš„ä½ç½®å‘é‡ï¼Œè¿™ä¸ªå‘é‡çš„ç¬¬iä¸ªå…ƒç´ çš„æ•°å€¼å°±æ˜¯$PE_i(p)$ã€‚æ³¢é•¿æ˜¯ä»$2\piåˆ°1000*2\pi$çš„å‡ ä½•çº§æ•°ã€‚
ä¹‹æ‰€ä»¥é€‰æ‹©è¿™ä¸ªå‡½æ•°ï¼Œç”±äºæˆ‘ä»¬æœ‰sin(Î±+Î²)=sinÎ±cosÎ²+cosÎ±sinÎ²ä»¥åŠcos(Î±+Î²)=cosÎ±cosÎ²âˆ’sinÎ±sinÎ²ï¼Œè¿™è¡¨æ˜ä½ç½®p+kçš„å‘é‡å¯ä»¥è¡¨ç¤ºæˆä½ç½®pçš„å‘é‡çš„çº¿æ€§å˜æ¢ï¼Œè¿™æä¾›äº†è¡¨è¾¾ç›¸å¯¹ä½ç½®ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚
å¯¹äºä»»ä½•åç§»é‡kï¼Œ$PE_{pos+k}$ å¯ä»¥è¡¨ç¤ºä¸º$PE_{pos}$çš„çº¿æ€§å‡½æ•°ã€‚

é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨encoderå’Œdecoderä¸­çš„positionalç¼–ç ä¸­å’Œembeddingsæ±‚å’Œä¸­éƒ½ä½¿ç”¨äº†dropoutã€‚å¯¹äºbase modelï¼Œ$P_{drop} = 0.1$


```python
class PositionalEncoding(nn.Module):
    """PEå‡½æ•°å®ç°"""
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-(math.log(10000.0)/ d_model)))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)
        return self.dropout(x)
    
```


```python
plt.figure(figsize=(15, 5))
pe = PositionalEncoding(20, 0)
y = pe.forward(Variable(torch.zeros(1, 100, 20)))
plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())
plt.legend(["dim %d" %p for p in [4, 5, 6, 7]])
None
```


![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_36_0.png)


è®ºæ–‡é‡Œæ¯”è¾ƒè¿‡ç›´æ¥è®­ç»ƒå‡ºæ¥çš„ä½ç½®å‘é‡å’Œä¸Šè¿°å…¬å¼è®¡ç®—å‡ºæ¥çš„ä½ç½®å‘é‡ï¼Œæ•ˆæœæ˜¯æ¥è¿‘çš„ã€‚å› æ­¤æ˜¾ç„¶æˆ‘ä»¬æ›´ä¹æ„ä½¿ç”¨å…¬å¼æ„é€ çš„Position Embeddingäº†ï¼Œå› ä¸ºå…è®¸æ¨¡å‹æ‰©å±•åˆ°æ¯”è®­ç»ƒæ—¶å€™åºåˆ—æ›´é•¿çš„åºåˆ—é•¿åº¦ã€‚


```python
def make_model(src_vacab, tgt_vocab, N=6, d_model=512, d_ff =2048, h=8, dropout=0.1):
    """ æ„å»ºæ¨¡å‹"""
    c = copy.deepcopy
    attn = MultiHeadedAttention(h, d_model)
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)
    position = PositionalEncoding(d_model, dropout)
    model = EncoderDecoder(
        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),
        nn.Sequential(Embeddings(d_model, src_vacab), c(position)),
        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),
        Generator(d_model, tgt_vocab)
    )
    
    # !!!import for the work
    # ä½¿ç”¨Glorot/ fan_avgåˆå§‹åŒ–å‚æ•°
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform(p)
    return model
```


```python
tmp_model = make_model(10, 10, 2)
```


```python
tmp_model
```




    EncoderDecoder(
      (encoder): Encoder(
        (layers): ModuleList(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm(
                )
                (dropout): Dropout(p=0.1)
              )
              (1): SublayerConnection(
                (norm): LayerNorm(
                )
                (dropout): Dropout(p=0.1)
              )
            )
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm(
                )
                (dropout): Dropout(p=0.1)
              )
              (1): SublayerConnection(
                (norm): LayerNorm(
                )
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (norm): LayerNorm(
        )
      )
      (decoder): Decoder(
        (layers): ModuleList(
          (0): DecoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1)
            )
            (src_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm(
                )
                (dropout): Dropout(p=0.1)
              )
              (1): SublayerConnection(
                (norm): LayerNorm(
                )
                (dropout): Dropout(p=0.1)
              )
              (2): SublayerConnection(
                (norm): LayerNorm(
                )
                (dropout): Dropout(p=0.1)
              )
            )
          )
          (1): DecoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1)
            )
            (src_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm(
                )
                (dropout): Dropout(p=0.1)
              )
              (1): SublayerConnection(
                (norm): LayerNorm(
                )
                (dropout): Dropout(p=0.1)
              )
              (2): SublayerConnection(
                (norm): LayerNorm(
                )
                (dropout): Dropout(p=0.1)
              )
            )
          )
        )
        (norm): LayerNorm(
        )
      )
      (src_embed): Sequential(
        (0): Embeddings(
          (lut): Embedding(10, 512)
        )
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1)
        )
      )
      (tgt_embed): Sequential(
        (0): Embeddings(
          (lut): Embedding(10, 512)
        )
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1)
        )
      )
      (generator): Generator(
        (proj): Linear(in_features=512, out_features=10, bias=True)
      )
    )



## æ¨¡å‹è®­ç»ƒ
é¦–å…ˆå®šä¹‰ä¸€ä¸ªåŒ…å«æºå¥å­å’Œç›®æ ‡å¥å­çš„æ‰¹å¤„ç†å¯¹è±¡ï¼ŒåŒäº‹æ„å»ºmasks

### Batches and Masking


```python
class Batch:
    """ åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨maskå¤„ç†æ•°æ® """
    def __init__(self, src, trg=None, pad=0):
        self.src = src
        self.src_mask = (src != pad).unsqueeze(-2)
        if trg is not None:
            self.trg = trg[:, :-1]
            self.trg_y = trg[:, 1:]
            self.trg_mask = self.make_std_mask(self.trg, pad)
            self.ntokens = (self.trg_y != pad).data.sum()
    
    @staticmethod
    def make_std_mask(tgt, pad):
        """ åˆ›é€ ä¸€ä¸ªmaskæ¥å±è”½è¡¥å…¨è¯å’Œå­—å…¸å¤–çš„è¯è¿›è¡Œå±è”½"""
        tgt_mask = (tgt != pad).unsqueeze(-2)
        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))
        return tgt_mask
```

### Training and Loss compute


```python
def run_epoch(data_iter, model, loss_compute):
    """ æ ‡å‡†è®­ç»ƒå’Œæ—¥å¿—å‡½æ•° """
    start = time.time()
    total_tokens = 0
    total_loss = 0
    tokens = 0
    for i, batch in enumerate(data_iter):
        out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)
        loss = loss_compute(out, batch.trg_y, batch.ntokens)
        total_loss += loss
        total_tokens += batch.ntokens
        tokens += batch.ntokens
        if i % 50 == 1:
            elapsed = time.time() - start
            print("Epoch Step: %d Loss : %f Tokens per Sec: %f " % (i, loss/ batch.ntokens, tokens / elapsed))
            start = time.time()
            tokens = 0
    return total_loss / total_tokens
```

### Training Data and Batching
è®ºæ–‡ä¸­ä½¿ç”¨çš„æ•°æ®é›†è¾ƒå¤§ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯torchtextå‡½æ•°


```python
global max_src_in_batch, max_tgt_in_batch
def batch_size_fn(new, count, sofar):
    """ ä¿æŒæ•°æ®æ‰¹é‡å¢åŠ ï¼Œå¹¶è®¡ç®—tokens+paddingçš„æ€»æ•° """
    global max_src_in_batch, max_tgt_in_batch
    if count == 1:
        max_src_in_batch = 0
        max_tgt_in_batch = 0
    
    max_src_in_batch = max(max_src_in_batch, len(new.src))
    max_tgt_in_batch = max(max_tgt_in_batch, len(new.trg) + 2)
    
    src_elements = count * max_src_in_batch
    tgt_elements = count * max_tgt_in_batch
    return max(src_elements, tgt_elements)
```

### Optimizer
ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œ å…¶ä¸­$\beta_1 = 0.9, \beta_2 = 0.98, \epsilon = 10^{-1}$

ä½¿ç”¨å¦‚ä¸‹æ–¹å¼è°ƒæ•´å­¦ä¹ ç‡ï¼š
$$lrate = d_{model}^{-0.5}\cdot \min(step\_num^{-0.5}, step\_num \cdot warmup\_step^{-1.5})$$
å…ˆéšç€è®­ç»ƒstepçº¿æ€§å¢åŠ ï¼Œä¹‹åå°†å…¶ä¸æ­¥æ•°çš„å€’æ•°å¹³æ–¹æ ¹æˆæ¯”ä¾‹åœ°å‡å°ï¼Œè®ºæ–‡ä¸­warmupsteps = 4000


```python
# è¿™ä¸ªéƒ¨åˆ†å¾ˆé‡è¦ï¼Œéœ€è¦è¿™æ ·è®¾ç½®æ¨¡å‹å‚æ•°
```


```python
class NoamOpt:
    def __init__(self, model_size, factor, warmup, optimizer):
        self.optimizer = optimizer
        self._step = 0
        self.warmup = warmup
        self.factor = factor
        self.model_size = model_size
        self._rate = 0
        
    def step(self):
        """ æ›´æ–°å‚æ•°å’Œå­¦ä¹ ç‡ """
        self._step += 1
        rate = self.rate()
        for p in self.optimizer.param_groups:
            p['lr'] = rate
        self._rate = rate
        self.optimizer.step()
        
    def rate(self, step = None):
        """ lrate å®ç°"""
        if step is None:
            step = self._step
        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))
    
def get_std_up(model):
    return NoamOpt(model.src_embed[0].d_model, 2, 4000, 
                   torch.optim.Adam(model.param_groups(), 
                                    lr = 0, betas = (0.9, 0.98), eps = 1e-9))
```

#### é’ˆå¯¹ä¸åŒæ¨¡å‹å¤§å°å’Œä¼˜åŒ–è¶…å‚æ•°çš„æ­¤æ¨¡å‹çš„æ›²çº¿ç¤ºä¾‹ã€‚


```python
opts = [NoamOpt(512, 1, 4000, None),
       NoamOpt(512, 1, 8000, None),
       NoamOpt(256, 1, 4000, None)]

plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])
plt.legend(["512:4000", "512:8000", "256:4000"])
None
```


![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_52_0.png)


### Regularization
#### Label Smoothing æ ‡ç­¾å¹³æ»‘
è®ºæ–‡ä¸­è®­ç»ƒçš„æ—¶å€™ä½¿ç”¨$\epsilon = 0.1$è¿›è¡Œæ ‡ç­¾å¹³æ»‘ï¼ˆ[å‚è€ƒè®ºæ–‡](https://arxiv.org/abs/1512.00567)ï¼‰ï¼Œè¿™æ ·ä¼šå¢åŠ å¤æ‚æ€§ï¼Œå› ä¸ºæ¨¡å‹å­¦å¾—æ›´åŠ ä¸ç¡®å®šæ€§ï¼Œä½†æé«˜äº†å‡†ç¡®æ€§å’ŒBLEUåˆ†æ•°ã€‚

åœ¨å®é™…å®ç°æ—¶ï¼Œè¿™é‡Œä½¿ç”¨KL div losså®ç°æ ‡ç­¾å¹³æ»‘ã€‚æ²¡æœ‰ä½¿ç”¨one-hotç›®æ ‡åˆ†å¸ƒï¼Œè€Œæ˜¯åˆ›å»ºäº†ä¸€ä¸ªåˆ†å¸ƒï¼Œå¯¹äºæ•´ä¸ªè¯æ±‡åˆ†å¸ƒè¡¨ï¼Œè¿™ä¸ªåˆ†å¸ƒå«æœ‰æ­£ç¡®å•è¯åº¦å’Œå‰©ä½™éƒ¨åˆ†å¹³æ»‘å—çš„ç½®ä¿¡åº¦


```python
class LabelSmoothing(nn.Module):
    """ æ ‡ç­¾å¹³æ»‘å®ç° """
    def __init__(self, size, padding_idx, smoothing=0.0):
        super(LabelSmoothing, self).__init__()
        self.criterion = nn.KLDivLoss(size_average=False)
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
        self.true_dist = None
        
    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() > 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
            
        self.true_dist = true_dist
        return self.criterion(x, Variable(true_dist, requires_grad=False))            
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åŸºäºç½®ä¿¡åº¦å¦‚ä½•å°†è¯­æ–™åˆ†å¸ƒåˆ°å•è¯çš„ç¤ºä¾‹ã€‚


```python
# æ ‡ç­¾å¹³æ»‘çš„ä¾‹å­
crit = LabelSmoothing(5, 0, 0.4)
predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],
                            [0, 0.2, 0.7, 0.1, 0],
                            [0, 0.2, 0.7, 0.1, 0]])

v = crit(Variable(predict.log()), Variable(torch.LongTensor([2, 1, 0])))
# å±•ç¤ºç›®æ ‡labelçš„æœŸæœ›åˆ†å¸ƒ
plt.imshow(crit.true_dist)
None
```


![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_56_0.png)


#### å¦‚æœå¯¹äºä¸€ä¸ªç»™å®šé€‰æ‹©éå¸¸æœ‰ä¿¡æ¯ï¼Œæ ‡ç­¾å¹³æ»‘å®é™…ä¸Šå¼€å§‹æƒ©ç½šæ¨¡å‹



```python
crit = LabelSmoothing(5, 0, 0.1)
def loss(x):
    d = x + 3 * 1
    predict = torch.FloatTensor([[0, x/d, 1/d, 1/d, 1/d],])
    return crit(Variable(predict.log()),
               Variable(torch.LongTensor([1]))).data[0]
plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])
None
```


![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_58_0.png)


### å®æˆ˜ï¼šç¬¬ä¸€ä¸ªä¾‹å­
ç»™å®šä¸€ä¸ªæ¥è‡ªå°è¯æ±‡è¡¨çš„éšæœºè¾“å…¥ç¬¦å·é›†ï¼Œç›®æ ‡æ˜¯ç”Ÿæˆç›¸åŒçš„ç¬¦å·

### åˆæˆæ•°æ®


```python
def data_gen(V, batch, nbatches):
    """ ç”Ÿæˆä¸€ä¸ªéšæœºæ•°æ®ç”¨äº src-tgtå¤åˆ¶ä»»åŠ¡"""
    for i in range(nbatches):
        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))
        data[:, 0] = 1
        src = Variable(data, requires_grad = False)
        tgt = Variable(data, requires_grad = False)
        yield Batch(src, tgt, 0)
```

### æŸå¤±è®¡ç®—


```python
class SimpleLossCompute:
    def __init__(self, generator, criterion, opt=None):
        self.generator = generator
        self.criterion = criterion
        self.opt = opt
        
    def __call__(self, x, y, norm):
        x = self.generator(x)        
        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),
                             y.contiguous().view(-1)) / norm
        loss.backward()
        if self.opt is not None:
            self.opt.step()
            self.opt.optimizer.zero_grad()
        return loss.data[0] * norm
```

### è´ªå¿ƒDecoding


```python
V = 11
criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)
model = make_model(V,V, N=2)
model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400, 
                    torch.optim.Adam(model.parameters(), lr=0, betas= (0.9, 0.98), eps = 1e-9))

for epoch in range(10):
    model.train()
    run_epoch(data_gen(V, 30, 20), model, SimpleLossCompute(model.generator, criterion, model_opt))
    model.eval()
    print(run_epoch(data_gen(V, 30, 5), model, SimpleLossCompute(model.generator, criterion, None)))
```

    Epoch Step: 1 Loss : 3.328415 Tokens per Sec: 576.238429 
    Epoch Step: 1 Loss : 1.918249 Tokens per Sec: 1231.307105 
    1.9602290630340575
    Epoch Step: 1 Loss : 1.989585 Tokens per Sec: 628.817043 
    Epoch Step: 1 Loss : 1.696718 Tokens per Sec: 950.082767 
    1.7239755392074585
    Epoch Step: 1 Loss : 1.785850 Tokens per Sec: 634.098792 
    Epoch Step: 1 Loss : 1.525769 Tokens per Sec: 1203.840811 
    1.5165136814117433
    Epoch Step: 1 Loss : 1.778025 Tokens per Sec: 653.846467 
    Epoch Step: 1 Loss : 1.222946 Tokens per Sec: 1136.071379 
    1.1622309684753418
    Epoch Step: 1 Loss : 1.238613 Tokens per Sec: 668.932793 
    Epoch Step: 1 Loss : 0.997262 Tokens per Sec: 1121.050903 
    1.0589515805244445
    Epoch Step: 1 Loss : 1.218332 Tokens per Sec: 655.329661 
    Epoch Step: 1 Loss : 0.739608 Tokens per Sec: 1086.835112 
    0.7851933479309082
    Epoch Step: 1 Loss : 0.782321 Tokens per Sec: 616.218030 
    Epoch Step: 1 Loss : 0.397338 Tokens per Sec: 1076.720783 
    0.4492629885673523
    Epoch Step: 1 Loss : 0.600206 Tokens per Sec: 641.399357 
    Epoch Step: 1 Loss : 0.224609 Tokens per Sec: 1124.049134 
    0.299098813533783
    Epoch Step: 1 Loss : 0.451857 Tokens per Sec: 631.001445 
    Epoch Step: 1 Loss : 0.361520 Tokens per Sec: 916.635636 
    0.3899755597114563
    Epoch Step: 1 Loss : 0.986942 Tokens per Sec: 664.370915 
    Epoch Step: 1 Loss : 0.413342 Tokens per Sec: 1099.376349 
    0.3377967059612274


ä¸ºäº†ç®€å•èµ·è§ï¼Œä½¿ç”¨è´ªå©ªçš„è§£ç æ–¹å¼æ¥é¢„æµ‹å¯¹åº”ç¿»è¯‘


```python
def greedy_decode(model, src, src_mask, max_len, start_symbol):
    memory = model.encode(src, src_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)
    for i in range(max_len - 1):
        out = model.decode(memory, src_mask,
                          Variable(ys),
                          Variable(subsequent_mask(ys.size(1)).type_as(src.data)))
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim = 1)
        next_word = next_word.data[0]
        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)
    return ys
model.eval()
src = Variable(torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]))
src_mask = Variable(torch.ones(1, 1, 10))
print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))
```

    
        1     2     3     4     5     8     6     7     9    10
    [torch.LongTensor of size 1x10]
    


## çœŸå®çš„ä¾‹å­
ä¸‹é¢ä½¿ç”¨the IWSLT German-English Translation taskåšä¸ºçœŸå®ä»»åŠ¡ã€‚è¿™ä¸ªä»»åŠ¡è¦æ¯”åŸå§‹è®ºæ–‡ä¸­çš„VMTä»»åŠ¡å°çš„å¤šï¼Œä½†æ˜¯å®ƒä¹Ÿèƒ½ååº”æ•´ä¸ªç³»ç»Ÿçš„ä¼˜åŠ¿ã€‚è¿™é‡Œä¹Ÿä»‹ç»äº†å¦‚ä½•ä½¿ç”¨å¤šGPUç¼–ç¨‹ä½¿å¾—å®ƒé€Ÿåº¦æ›´å¿«

### Data Loading
è¿™é‡Œä½¿ç”¨torchtextåŠ è½½æ•°æ®å’Œspacyè¿›è¡Œtokenåˆ‡åˆ†


```python
from torchtext import data, datasets

if True:
    import spacy
    spacy_de = spacy.load('de')
    spacy_en = spacy.load('en')
    
    def tokenize_de(text):
        return [tok.text for tok in spacy_de.tokenizer(text)]
    
    def tokenize_en(text):
        return [tok.text for tok in spacy_en.tokenizer(text)]
    
    BOS_WORD = '<s>'
    EOS_WORD = '</S>'
    BLANK_WORD = "<blank>"
    SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)
    TGT = data.Field(tokenize=tokenize_en, init_token=BOS_WORD, eos_token=EOS_WORD, pad_token=BLANK_WORD)
    
    MAX_LEN = 100
    train, val, test = datasets.IWSLT.splits(
        exts=('.de', '.en'), fields=(SRC, TGT),
        filter_pred=lambda x : len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)
    MIN_FREQ = 2
    SRC.build_vocab(train.src, min_freq=MIN_FREQ)
    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)
    
```

    
    [93m    Warning: no model found for 'de'[0m
    
        Only loading the 'de' tokenizer.
    
    
    [93m    Warning: no model found for 'en'[0m
    
        Only loading the 'en' tokenizer.
    


åˆ†æ‰¹æ¬¡å¯¹äºè®­ç»ƒé€Ÿåº¦æ¥è¯´éå¸¸é‡è¦ã€‚æˆ‘ä»¬å¸Œæœ›æ‹¥æœ‰éå¸¸å‡åŒ€åˆ†å‰²çš„æ‰¹æ¬¡ï¼ŒåŒäº‹æ‹¥æœ‰æœ€å°çš„å¡«å……ï¼ˆpaddingï¼‰ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œéœ€è¦ä¿®æ”¹torchtextçš„é»˜è®¤çš„æ‰¹å¤„ç†æ–¹å¼ã€‚ä¸‹é¢çš„ä»£ç ä¿®æ”¹å…¶é»˜è®¤çš„æ‰¹å¤„ç†æ–¹å¼ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬èƒ½å¤Ÿæœç´¢è¶³å¤Ÿå¤šçš„å¥å­ä»¥æ‰¾åˆ°å¥½çš„æ‰¹å¤„ç†ã€‚

### Iterators


```python
class MyIterator(data.Iterator):
    def create_batches(self):
        if self.train:
            def pool(d, random_shuffler):
                for p in data.batch(d, self.batch_size * 100):
                    p_batch = data.batch(
                        sorted(p, key=self.sort_key),
                        self.batch_size, self.batch_size_fn)
                    for b in random_shuffler(list(p_batch)):
                        yield b
            self.batches = pool(self.data(), self.random_shuffler)
            
        else:
            self.batches = []
            for b in data.batch(self.data(), self.batch_size,
                                          self.batch_size_fn):
                self.batches.append(sorted(b, key=self.sort_key))

def rebatch(pad_idx, batch):
    "Fix order in torchtext to match ours"
    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)
    return Batch(src, trg, pad_idx)

```

### Multi-GPU Training
ä¸ºäº†åŠ é€Ÿè®­ç»ƒï¼Œä½¿ç”¨å¤šGPUã€‚å®ç°æ–¹æ³•ä¸æ˜¯transformerç‹¬æœ‰çš„ï¼Œæ‰€ä»¥ç»†èŠ‚ä¸Šä¸å¤šåšä»‹ç»ã€‚å®ç°æ–¹æ³•æ˜¯åœ¨è®­ç»ƒæ—¶å°†å•è¯åˆ†å—åˆ†é…åˆ°ä¸é€šçš„GPUä¸Šï¼Œä»¥ä¾¿å¹¶è¡Œå¤„ç†ã€‚æˆ‘ä»¬ä½¿ç”¨pytorchçš„å¹¶è¡Œå¤„ç†åŸè¯­æ¥å®ç°ã€‚

- replicate - å°†æ¨¡å—æ‹†åˆ†åˆ°ä¸åŒçš„GPUä¸Š 
- scatter - å°†æ‰¹æ¬¡æ‹†åˆ†åˆ°ä¸åŒçš„GPUä¸Š
- parallel_apply -  å°†æ¨¡å—åº”ç”¨äºä½äºä¸åŒgpusä¸Šçš„æ‰¹æ¬¡
- gather - å°†åˆ†æ•£çš„æ•°æ®æ‹‰å›åˆ°ä¸€ä¸ªgpuä¸Šã€‚
- nn.DataParallel - ä¸€ä¸ªç‰¹æ®Šçš„æ¨¡å—åŒ…è£…å™¨ï¼Œåœ¨è¯„ä¼°ä¹‹å‰è°ƒç”¨ã€‚


```python
class MultiGPULossCompute:
    "A multi-gpu loss compute and train function."
    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):
        # Send out to different gpus.
        self.generator = generator
        self.criterion = nn.parallel.replicate(criterion, 
                                               devices=devices)
        self.opt = opt
        self.devices = devices
        self.chunk_size = chunk_size
        
    def __call__(self, out, targets, normalize):
        total = 0.0
        generator = nn.parallel.replicate(self.generator, 
                                                devices=self.devices)
        out_scatter = nn.parallel.scatter(out, 
                                          target_gpus=self.devices)
        out_grad = [[] for _ in out_scatter]
        targets = nn.parallel.scatter(targets, 
                                      target_gpus=self.devices)

        # Divide generating into chunks.
        chunk_size = self.chunk_size
        for i in range(0, out_scatter[0].size(1), chunk_size):
            # Predict distributions
            out_column = [[Variable(o[:, i:i+chunk_size].data, 
                                    requires_grad=self.opt is not None)] 
                           for o in out_scatter]
            gen = nn.parallel.parallel_apply(generator, out_column)

            # Compute loss. 
            y = [(g.contiguous().view(-1, g.size(-1)), 
                  t[:, i:i+chunk_size].contiguous().view(-1)) 
                 for g, t in zip(gen, targets)]
            loss = nn.parallel.parallel_apply(self.criterion, y)

            # Sum and normalize loss
            l = nn.parallel.gather(loss, 
                                   target_device=self.devices[0])
            l = l.sum()[0] / normalize
            total += l.data[0]

            # Backprop loss to output of transformer
            if self.opt is not None:
                l.backward()
                for j, l in enumerate(loss):
                    out_grad[j].append(out_column[j][0].grad.data.clone())

        # Backprop all loss through transformer.            
        if self.opt is not None:
            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]
            o1 = out
            o2 = nn.parallel.gather(out_grad, 
                                    target_device=self.devices[0])
            o1.backward(gradient=o2)
            self.opt.step()
            self.opt.optimizer.zero_grad()
        return total * normalize
```


```python
# éœ€è¦ä½¿ç”¨çš„GPU
devices = [0] # å¦‚æœåªæœ‰ä¸€ä¸ªGPUï¼Œä½¿ç”¨devices=[0]
if True:
    pad_idx = TGT.vocab.stoi["<blank>"]
    model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)
    model.cuda()
    criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)
    criterion.cuda()
    BATCH_SIZE = 12000
    train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0, repeat=False, 
                            sort_key=lambda x : (len(x.src), len(x.trg)),
                           batch_size_fn=batch_size_fn, train=True)
    valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0, repeat=False,
                           sort_key=lambda x : (len(x.src), len(x.trg)),
                           batch_size_fn=batch_size_fn, train=False)
    model_par = nn.DataParallel(model, device_ids=devices)
None
# è¿™é‡Œéœ€è¦å¾ˆå¤§çš„å†…å­˜ï¼ŒæŠ¥å†…å­˜é”™è¯¯å¾ˆæ­£å¸¸ï¼Œå¯ä»¥ç›´æ¥ç”¨ä¸‹é¢è®­ç»ƒå¥½çš„
# æˆ–è€…è°ƒå°BATCH_SIZE
```

### è®­ç»ƒæ¨¡å‹


```python
!wget https://s3.amazonaws.com/opennmt-models/iwslt.pt
```

    --2018-11-23 10:41:12--  https://s3.amazonaws.com/opennmt-models/iwslt.pt
    Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.99.165
    Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.99.165|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 467317581 (446M) [application/x-www-form-urlencoded]
    Saving to: â€˜iwslt.ptâ€™
    
    iwslt.pt            100%[===================>] 445.67M  4.19MB/s    in 2m 6s   
    
    2018-11-23 10:43:20 (3.52 MB/s) - â€˜iwslt.ptâ€™ saved [467317581/467317581]
    



```python
if True:
    model_opt = NoamOpt(model.src_embed[0].d_model, 1, 2000,
                       torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))
    for epoch in range(10):
        model_par.train()
        run_epoch((rebatch(pad_idx, b) for b in train_iter),
                  model_par, 
                  MultiGPULossCompute(model.generator, criterion, devices=devices, opt=model_opt))
        model_par.eval()
        loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), 
                         model_par, 
                         MultiGPULossCompute(model.generator, criterion, devices=devices, opt=None))
        print("loss is: %f" % loss)
else:
    model = torch.load("iwslt.pt")
    
```

    Epoch Step: 1 Loss : 9.115798 Tokens per Sec: 27.246552 
    Epoch Step: 51 Loss : 8.795138 Tokens per Sec: 112.815511 
    Epoch Step: 101 Loss : 8.123654 Tokens per Sec: 111.014568 
    Epoch Step: 151 Loss : 6.664777 Tokens per Sec: 116.761415 
    Epoch Step: 201 Loss : 6.139312 Tokens per Sec: 121.009845 
    Epoch Step: 251 Loss : 6.167897 Tokens per Sec: 108.962893 
    Epoch Step: 301 Loss : 6.259430 Tokens per Sec: 132.455119 
    Epoch Step: 351 Loss : 6.787484 Tokens per Sec: 112.652976 
    Epoch Step: 401 Loss : 5.871879 Tokens per Sec: 116.497260 
    Epoch Step: 451 Loss : 5.735006 Tokens per Sec: 119.585556 
    Epoch Step: 501 Loss : 5.359651 Tokens per Sec: 124.397652 
    Epoch Step: 551 Loss : 5.068015 Tokens per Sec: 123.167267 
    Epoch Step: 601 Loss : 4.455752 Tokens per Sec: 125.204276 
    Epoch Step: 651 Loss : 5.691089 Tokens per Sec: 123.898633 
    Epoch Step: 701 Loss : 4.096532 Tokens per Sec: 116.935452 
    Epoch Step: 751 Loss : 5.427895 Tokens per Sec: 113.504319 
    Epoch Step: 801 Loss : 4.077328 Tokens per Sec: 114.052573 
    Epoch Step: 851 Loss : 5.073078 Tokens per Sec: 124.525615 
    Epoch Step: 901 Loss : 6.252522 Tokens per Sec: 111.752486 
    Epoch Step: 951 Loss : 5.530307 Tokens per Sec: 122.574079 
    Epoch Step: 1001 Loss : 7.020243 Tokens per Sec: 112.812934 
    Epoch Step: 1051 Loss : 6.083723 Tokens per Sec: 108.350083 
    Epoch Step: 1101 Loss : 5.993296 Tokens per Sec: 125.369539 
    Epoch Step: 1151 Loss : 5.358730 Tokens per Sec: 113.058275 
    Epoch Step: 1201 Loss : 5.063932 Tokens per Sec: 123.037357 
    Epoch Step: 1251 Loss : 4.229568 Tokens per Sec: 130.335474 
    Epoch Step: 1301 Loss : 4.840786 Tokens per Sec: 125.532243 
    Epoch Step: 1351 Loss : 4.929336 Tokens per Sec: 122.771668 
    Epoch Step: 1401 Loss : 4.490815 Tokens per Sec: 117.422497 
    Epoch Step: 1451 Loss : 5.564615 Tokens per Sec: 113.588785 
    Epoch Step: 1501 Loss : 6.767705 Tokens per Sec: 113.181958 
    Epoch Step: 1551 Loss : 5.262781 Tokens per Sec: 109.249956 
    Epoch Step: 1601 Loss : 5.217732 Tokens per Sec: 124.954539 
    Epoch Step: 1651 Loss : 6.329094 Tokens per Sec: 110.064008 
    Epoch Step: 1701 Loss : 4.028631 Tokens per Sec: 121.401211 
    Epoch Step: 1751 Loss : 5.967369 Tokens per Sec: 110.828476 
    Epoch Step: 1801 Loss : 5.388975 Tokens per Sec: 123.864972 
    Epoch Step: 1851 Loss : 6.298844 Tokens per Sec: 126.828853 
    Epoch Step: 1901 Loss : 5.405447 Tokens per Sec: 126.701156 
    Epoch Step: 1951 Loss : 6.067888 Tokens per Sec: 107.605533 
    Epoch Step: 2001 Loss : 6.092531 Tokens per Sec: 112.409601 
    Epoch Step: 2051 Loss : 5.238168 Tokens per Sec: 124.090227 
    Epoch Step: 2101 Loss : 6.180753 Tokens per Sec: 124.890180 
    Epoch Step: 2151 Loss : 5.203738 Tokens per Sec: 116.304379 
    Epoch Step: 2201 Loss : 5.281256 Tokens per Sec: 114.753139 
    Epoch Step: 2251 Loss : 5.884039 Tokens per Sec: 132.970013 
    Epoch Step: 2301 Loss : 5.384135 Tokens per Sec: 116.574694 
    Epoch Step: 2351 Loss : 4.522669 Tokens per Sec: 116.200856 
    Epoch Step: 2401 Loss : 5.505316 Tokens per Sec: 136.415777 
    Epoch Step: 2451 Loss : 6.195974 Tokens per Sec: 105.207516 
    Epoch Step: 2501 Loss : 5.338635 Tokens per Sec: 111.450436 
    Epoch Step: 2551 Loss : 6.587592 Tokens per Sec: 120.631876 
    Epoch Step: 2601 Loss : 5.582253 Tokens per Sec: 108.563041 
    Epoch Step: 2651 Loss : 6.719680 Tokens per Sec: 124.482789 
    Epoch Step: 2701 Loss : 6.663082 Tokens per Sec: 111.772424 
    Epoch Step: 2751 Loss : 6.241520 Tokens per Sec: 106.365521 
    Epoch Step: 2801 Loss : 6.269626 Tokens per Sec: 120.436012 
    Epoch Step: 2851 Loss : 5.431211 Tokens per Sec: 126.338207 
    Epoch Step: 2901 Loss : 4.765350 Tokens per Sec: 142.394894 
    Epoch Step: 2951 Loss : 5.357254 Tokens per Sec: 118.471949 
    Epoch Step: 3001 Loss : 5.476350 Tokens per Sec: 120.072949 
    Epoch Step: 3051 Loss : 4.086349 Tokens per Sec: 118.580939 
    Epoch Step: 3101 Loss : 5.351029 Tokens per Sec: 100.739349 
    Epoch Step: 3151 Loss : 4.154559 Tokens per Sec: 111.941574 
    Epoch Step: 3201 Loss : 6.364028 Tokens per Sec: 114.184378 
    Epoch Step: 3251 Loss : 5.718968 Tokens per Sec: 99.218677 
    Epoch Step: 3301 Loss : 5.274074 Tokens per Sec: 117.763638 
    Epoch Step: 3351 Loss : 5.384217 Tokens per Sec: 125.402623 
    Epoch Step: 3401 Loss : 5.846756 Tokens per Sec: 132.068638 
    Epoch Step: 3451 Loss : 5.346969 Tokens per Sec: 115.760696 
    Epoch Step: 3501 Loss : 4.732840 Tokens per Sec: 128.605812 
    Epoch Step: 3551 Loss : 5.474891 Tokens per Sec: 118.020504 
    Epoch Step: 3601 Loss : 4.852159 Tokens per Sec: 134.320444 
    Epoch Step: 3651 Loss : 5.451114 Tokens per Sec: 131.592711 
    Epoch Step: 3701 Loss : 5.287390 Tokens per Sec: 121.867178 
    Epoch Step: 3751 Loss : 4.070665 Tokens per Sec: 125.037012 
    Epoch Step: 3801 Loss : 4.605243 Tokens per Sec: 116.971368 
    Epoch Step: 3851 Loss : 5.196218 Tokens per Sec: 118.281831 
    Epoch Step: 3901 Loss : 5.077266 Tokens per Sec: 121.393991 
    Epoch Step: 3951 Loss : 3.693632 Tokens per Sec: 130.745685 
    Epoch Step: 4001 Loss : 6.163212 Tokens per Sec: 115.424976 
    Epoch Step: 4051 Loss : 4.347610 Tokens per Sec: 127.553092 
    Epoch Step: 4101 Loss : 5.545203 Tokens per Sec: 127.837643 
    Epoch Step: 4151 Loss : 3.849491 Tokens per Sec: 110.226187 
    Epoch Step: 4201 Loss : 3.980309 Tokens per Sec: 114.186194 
    Epoch Step: 4251 Loss : 5.881281 Tokens per Sec: 101.449358 
    Epoch Step: 4301 Loss : 5.177263 Tokens per Sec: 120.402445 
    Epoch Step: 4351 Loss : 5.984916 Tokens per Sec: 123.156741 
    Epoch Step: 4401 Loss : 5.315350 Tokens per Sec: 127.725499 
    Epoch Step: 4451 Loss : 5.140157 Tokens per Sec: 120.348082 
    Epoch Step: 4501 Loss : 4.589716 Tokens per Sec: 115.936711 
    Epoch Step: 4551 Loss : 5.617508 Tokens per Sec: 123.089776 
    Epoch Step: 4601 Loss : 5.993092 Tokens per Sec: 105.903955 
    Epoch Step: 4651 Loss : 4.702520 Tokens per Sec: 130.786161 
    Epoch Step: 4701 Loss : 3.969887 Tokens per Sec: 108.160083 
    Epoch Step: 4751 Loss : 4.690508 Tokens per Sec: 113.869501 
    Epoch Step: 4801 Loss : 6.810272 Tokens per Sec: 113.617623 
    Epoch Step: 4851 Loss : 6.173089 Tokens per Sec: 126.269595 
    Epoch Step: 4901 Loss : 6.111300 Tokens per Sec: 125.321265 
    Epoch Step: 4951 Loss : 4.817343 Tokens per Sec: 119.167878 
    Epoch Step: 5001 Loss : 4.348187 Tokens per Sec: 118.838030 
    Epoch Step: 5051 Loss : 5.152521 Tokens per Sec: 137.800332 
    Epoch Step: 5101 Loss : 4.129012 Tokens per Sec: 93.108974 
    Epoch Step: 5151 Loss : 3.953586 Tokens per Sec: 109.937495 
    Epoch Step: 5201 Loss : 5.148945 Tokens per Sec: 121.888139 
    Epoch Step: 5251 Loss : 4.364405 Tokens per Sec: 109.109888 
    Epoch Step: 5301 Loss : 5.165133 Tokens per Sec: 114.984078 
    Epoch Step: 5351 Loss : 3.795167 Tokens per Sec: 113.023181 
    Epoch Step: 5401 Loss : 6.682058 Tokens per Sec: 118.737053 
    Epoch Step: 5451 Loss : 4.287959 Tokens per Sec: 104.789962 
    Epoch Step: 5501 Loss : 4.816484 Tokens per Sec: 115.577422 
    Epoch Step: 5551 Loss : 5.741656 Tokens per Sec: 127.177478 
    Epoch Step: 5601 Loss : 5.204136 Tokens per Sec: 116.299978 
    Epoch Step: 5651 Loss : 4.231252 Tokens per Sec: 104.789153 
    Epoch Step: 5701 Loss : 4.711532 Tokens per Sec: 115.029154 
    Epoch Step: 5751 Loss : 5.010115 Tokens per Sec: 96.724771 
    Epoch Step: 5801 Loss : 5.679179 Tokens per Sec: 103.678304 
    Epoch Step: 5851 Loss : 4.840335 Tokens per Sec: 130.473356 
    Epoch Step: 5901 Loss : 6.077025 Tokens per Sec: 115.137809 
    Epoch Step: 5951 Loss : 4.814110 Tokens per Sec: 123.587690 
    Epoch Step: 6001 Loss : 5.069083 Tokens per Sec: 114.077725 
    Epoch Step: 6051 Loss : 4.493855 Tokens per Sec: 109.636051 
    Epoch Step: 6101 Loss : 4.939024 Tokens per Sec: 105.390777 
    Epoch Step: 6151 Loss : 5.475710 Tokens per Sec: 108.860868 
    Epoch Step: 6201 Loss : 4.954838 Tokens per Sec: 114.393112 
    Epoch Step: 6251 Loss : 4.933300 Tokens per Sec: 133.104268 
    Epoch Step: 6301 Loss : 3.424345 Tokens per Sec: 123.297081 
    Epoch Step: 6351 Loss : 4.743899 Tokens per Sec: 123.668512 
    Epoch Step: 6401 Loss : 5.173761 Tokens per Sec: 116.578303 
    Epoch Step: 6451 Loss : 5.513897 Tokens per Sec: 115.043922 
    Epoch Step: 6501 Loss : 4.724627 Tokens per Sec: 124.206271 
    Epoch Step: 6551 Loss : 6.129314 Tokens per Sec: 101.478472 
    Epoch Step: 6601 Loss : 5.877912 Tokens per Sec: 114.402926 
    Epoch Step: 6651 Loss : 5.689779 Tokens per Sec: 114.263450 
    Epoch Step: 6701 Loss : 4.473933 Tokens per Sec: 137.414472 
    Epoch Step: 6751 Loss : 4.558855 Tokens per Sec: 119.674591 
    Epoch Step: 6801 Loss : 5.443909 Tokens per Sec: 117.412783 
    Epoch Step: 6851 Loss : 5.363852 Tokens per Sec: 132.389490 
    Epoch Step: 6901 Loss : 5.363576 Tokens per Sec: 116.070900 
    Epoch Step: 6951 Loss : 4.576881 Tokens per Sec: 119.039487 
    Epoch Step: 7001 Loss : 5.497922 Tokens per Sec: 113.802638 
    Epoch Step: 7051 Loss : 6.701012 Tokens per Sec: 118.306036 
    Epoch Step: 7101 Loss : 2.813069 Tokens per Sec: 112.697843 
    Epoch Step: 7151 Loss : 4.721593 Tokens per Sec: 118.721749 
    Epoch Step: 7201 Loss : 4.454715 Tokens per Sec: 126.265449 
    Epoch Step: 7251 Loss : 5.204863 Tokens per Sec: 110.260285 
    Epoch Step: 7301 Loss : 6.093715 Tokens per Sec: 113.003313 
    Epoch Step: 7351 Loss : 3.692578 Tokens per Sec: 114.283425 
    Epoch Step: 7401 Loss : 4.344957 Tokens per Sec: 106.621936 
    Epoch Step: 7451 Loss : 5.794671 Tokens per Sec: 115.961877 
    Epoch Step: 7501 Loss : 4.730721 Tokens per Sec: 137.014758 
    Epoch Step: 7551 Loss : 5.345826 Tokens per Sec: 103.827868 
    Epoch Step: 7601 Loss : 4.509674 Tokens per Sec: 118.272483 
    Epoch Step: 7651 Loss : 4.735533 Tokens per Sec: 124.188938 
    Epoch Step: 7701 Loss : 4.103126 Tokens per Sec: 118.460493 
    Epoch Step: 7751 Loss : 5.854985 Tokens per Sec: 129.768230 
    Epoch Step: 7801 Loss : 6.111448 Tokens per Sec: 119.035855 
    Epoch Step: 7851 Loss : 5.163021 Tokens per Sec: 112.247344 
    Epoch Step: 7901 Loss : 4.288763 Tokens per Sec: 131.180708 
    Epoch Step: 7951 Loss : 5.100498 Tokens per Sec: 117.128035 
    Epoch Step: 8001 Loss : 5.268844 Tokens per Sec: 125.332613 
    Epoch Step: 8051 Loss : 4.714831 Tokens per Sec: 129.184088 
    Epoch Step: 8101 Loss : 4.888225 Tokens per Sec: 123.157112 
    Epoch Step: 8151 Loss : 3.944123 Tokens per Sec: 127.816168 
    Epoch Step: 8201 Loss : 4.901572 Tokens per Sec: 115.956655 
    Epoch Step: 8251 Loss : 4.748166 Tokens per Sec: 108.353690 
    Epoch Step: 8301 Loss : 4.617119 Tokens per Sec: 112.925223 
    Epoch Step: 8351 Loss : 5.873656 Tokens per Sec: 116.868465 
    Epoch Step: 8401 Loss : 5.075790 Tokens per Sec: 109.263429 
    Epoch Step: 8451 Loss : 4.742764 Tokens per Sec: 118.775604 
    Epoch Step: 8501 Loss : 5.432733 Tokens per Sec: 118.623984 
    Epoch Step: 8551 Loss : 5.449299 Tokens per Sec: 100.890450 
    Epoch Step: 8601 Loss : 4.598395 Tokens per Sec: 109.322259 
    Epoch Step: 8651 Loss : 4.820777 Tokens per Sec: 121.645203 
    Epoch Step: 8701 Loss : 5.348703 Tokens per Sec: 120.845176 
    Epoch Step: 8751 Loss : 4.150349 Tokens per Sec: 116.405742 
    Epoch Step: 8801 Loss : 5.885915 Tokens per Sec: 126.198338 
    Epoch Step: 8851 Loss : 5.417064 Tokens per Sec: 109.893480 
    Epoch Step: 8901 Loss : 4.649410 Tokens per Sec: 113.371114 
    Epoch Step: 8951 Loss : 5.403794 Tokens per Sec: 131.876288 
    Epoch Step: 9001 Loss : 4.591927 Tokens per Sec: 109.430485 
    Epoch Step: 9051 Loss : 4.999243 Tokens per Sec: 113.865406 
    Epoch Step: 9101 Loss : 4.565268 Tokens per Sec: 114.044977 
    Epoch Step: 9151 Loss : 4.651077 Tokens per Sec: 99.303425 
    Epoch Step: 9201 Loss : 5.085640 Tokens per Sec: 112.334779 
    Epoch Step: 9251 Loss : 4.164530 Tokens per Sec: 110.798513 
    Epoch Step: 9301 Loss : 4.909528 Tokens per Sec: 113.906289 
    Epoch Step: 9351 Loss : 5.305653 Tokens per Sec: 118.921177 
    Epoch Step: 9401 Loss : 5.303745 Tokens per Sec: 105.413858 
    Epoch Step: 9451 Loss : 5.203192 Tokens per Sec: 102.531900 



    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    <ipython-input-52-27ea9f2d920a> in <module>()
          6         run_epoch((rebatch(pad_idx, b) for b in train_iter),
          7                   model_par,
    ----> 8                   MultiGPULossCompute(model.generator, criterion, devices=devices, opt=model_opt))
          9         model_par.eval()
         10         loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), 


    <ipython-input-32-1a41203ea234> in run_epoch(data_iter, model, loss_compute)
          5     total_loss = 0
          6     tokens = 0
    ----> 7     for i, batch in enumerate(data_iter):
          8         out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)
          9         loss = loss_compute(out, batch.trg_y, batch.ntokens)


    <ipython-input-52-27ea9f2d920a> in <genexpr>(.0)
          4     for epoch in range(10):
          5         model_par.train()
    ----> 6         run_epoch((rebatch(pad_idx, b) for b in train_iter),
          7                   model_par,
          8                   MultiGPULossCompute(model.generator, criterion, devices=devices, opt=model_opt))


    ~/.conda/envs/pytorch3/lib/python3.6/site-packages/torchtext/data/iterator.py in __iter__(self)
        149                         minibatch.sort(key=self.sort_key, reverse=True)
        150                 yield Batch(minibatch, self.dataset, self.device,
    --> 151                             self.train)
        152             if not self.repeat:
        153                 return


    ~/.conda/envs/pytorch3/lib/python3.6/site-packages/torchtext/data/batch.py in __init__(self, data, dataset, device, train)
         25                 if field is not None:
         26                     batch = [getattr(x, name) for x in data]
    ---> 27                     setattr(self, name, field.process(batch, device=device, train=train))
         28 
         29     @classmethod


    ~/.conda/envs/pytorch3/lib/python3.6/site-packages/torchtext/data/field.py in process(self, batch, device, train)
        186         """
        187         padded = self.pad(batch)
    --> 188         tensor = self.numericalize(padded, device=device, train=train)
        189         return tensor
        190 


    ~/.conda/envs/pytorch3/lib/python3.6/site-packages/torchtext/data/field.py in numericalize(self, arr, device, train)
        315                 arr = arr.contiguous()
        316         else:
    --> 317             arr = arr.cuda(device)
        318             if self.include_lengths:
        319                 lengths = lengths.cuda(device)


    ~/.conda/envs/pytorch3/lib/python3.6/site-packages/torch/_utils.py in _cuda(self, device, async)
         67         else:
         68             new_type = getattr(torch.cuda, self.__class__.__name__)
    ---> 69             return new_type(self.size()).copy_(self, async)
         70 
         71 


    KeyboardInterrupt: 


ç»è¿‡è®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œè§£ç ä»è€Œè¿›è¡Œç¿»è¯‘ã€‚è¿™é‡Œæˆ‘ä»¬ç®€å•çš„å¯¹éªŒè¯é›†çš„ç¬¬ä¸€ä¸ªå¥å­è¿›è¡Œç¿»è¯‘ã€‚è¿™ä¸ªæ•°æ®é›†éå¸¸å°ï¼Œæ‰€ä»¥ä½¿ç”¨è´ªå©ªæœç´¢å‡†ç¡®åº¦éå¸¸é«˜ã€‚


```python
for i, batch in enumerate(valid_iter):
    src = batch.src.transpose(0, 1)[:1]
    src_mask = (src != SRC.vocab.stoi["<blank>"]).unsqueeze(-2)
    out = greedy_decode(model, src, src_mask, max_len=60, start_symbol=TGT.vocab.stoi["<s>"])
    print("Translation: ", end="\t")
    for i in range(1, out.size(1)):
        sym = TGT.vocab.itos[out[0, i]]
        if sym == "</s>":
            break
        print(sym, end=" ")
    print()    
    print("Target:", end="\t")
    for i in range(1, batch.trg.size(0)):
        sym = TGT.vocab.itos[batch.trg.data[i, 0]]
        if sym == '</s>':
            break
        print(sym, end=" ")
    print()
    break
```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-46-80b2c4ba57eb> in <module>()
    ----> 1 for i, batch in enumerate(valid_iter):
          2     src = batch.src.transpose(0, 1)[:1]
          3     src_mask = (src != SRC.vocab.stoi["<blank>"]).unsqueeze(-2)
          4     out = greedy_decode(model, src, src_mask, max_len=60, start_symbol=TGT.vocab.stoi["<s>"])
          5     print("Translation: ", end="\t")


    NameError: name 'valid_iter' is not defined


## å…¶ä»–éƒ¨åˆ†ï¼šBPEã€æœç´¢ã€å¹³å‡
åˆ°è¿™é‡Œå·²ç»æ±‰é«˜äº†transformeræ¨¡å‹çš„æ‰€æœ‰éƒ¨åˆ†ã€‚æœ‰å››ä¸ªéƒ¨åˆ†æ²¡æœ‰è¯¦ç»†ä»‹ç»ã€‚é™„åŠ çš„ç‰¹æ€§åœ¨[OpenNMT-py](https://github.com/opennmt/opennmt-py)æœ‰ä»‹ç»

1) BPE/ Word-piece: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨åº“æ¥é¦–å…ˆå°†æ•°æ®é¢„å¤„ç†ä¸ºå­å­—å•å…ƒã€‚å‚è€ƒRico Sennrichâ€™s çš„[subword-nmt](https://github.com/rsennrich/subword-nmt)å®ç°ã€‚è¿™äº›æ¨¡å‹å°†æ•°æ®è½¬æ¢ä¸ºå¦‚ä¸‹æ ¼å¼ï¼š

â–Die â–Protokoll datei â–kann â– heimlich â–per â–E - Mail â–oder â–FTP â–an â–einen â–bestimmte n â–EmpfÃ¤nger â–gesendet â–werden .

2ï¼‰å…±äº«Embeddings(Shared Embeddings): ä½¿ç”¨BPEä¼šå…±äº«vocabularyï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å…±äº«source / target / generatorä¹‹é—´çš„æƒé‡å‘é‡ã€‚å…·ä½“å¯ä»¥å‚è€ƒ[è¿™ç¯‡æ–‡çŒ®](https://arxiv.org/abs/1608.05859)ã€‚ä½¿ç”¨å¯ä»¥åƒä¸‹é¢è¿™æ ·ï¼š


```python
if False:
    model.src_embed[0].lut.weight = model.tgt_embeddings[0].lut.weight
    model.generator.lut.weight = model.tgt_embed[0].lut.weight
```

3) Beam Search: è¿™ä¸ªæœ‰ç‚¹å¤æ‚ã€‚å¯ä»¥å‚è€ƒ[OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/Beam.py)çš„pytorchå®ç°
4ï¼‰Model Averaging: è®ºæ–‡é‡Œå¹³å‡æœ€åKä¸ªæ£€æŸ¥ç‚¹ï¼ˆæ¨¡å‹ç»“æœï¼‰ä»¥è¾¾åˆ°ä¸€ä¸ªé›†æˆæ•ˆæœï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€å †æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ç…§å¦‚ä¸‹æ–¹å¼å»åšï¼š


```python
def average(model, models):
    """å¹³å‡æ¨¡å‹ï¼Œåˆ›å»ºä¸€ä¸ªæ–°æ¨¡å‹"""
    for ps in zip(*[m.params() for m in [model] + models])
    p[0].copy_(torch.sum(*ps[1:]) / len(ps[1:]))
```


      File "<ipython-input-53-6a7063a2cb31>", line 3
        for ps in zip(*[m.params() for m in [model] + models])
                                                              ^
    SyntaxError: invalid syntax



### ç»“æœ
è®ºæ–‡é‡Œä½¿ç”¨8ä¸ªP100 GPUSè®­ç»ƒäº†3.5å¤©ã€‚åœ¨WMT 2014 English-to-French translation taskä¸­è¾¾åˆ°æœ€å¥½æ•ˆæœã€‚dropoutä½¿ç”¨çš„æ˜¯0.1.


```python
Image(filename="images/result.png")
```




![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_86_0.png)



è¿™é‡Œçš„ä»£ç æ˜¯åŸºç¡€ç‰ˆæœ¬ï¼Œå®Œæ•´çš„ç³»ç»Ÿç‰ˆæœ¬å¯ä»¥å‚è€ƒ ([Example Models](http://opennmt.net/Models-py/)).
å¯ä»¥ä½¿ç”¨ä¸‹é¢è®­ç»ƒå¥½çš„æ¨¡å‹


```python
# !wget https://s3.amazonaws.com/opennmt-models/en-de-model.pt
```


```python
model, SRC, TGT = torch.load("en-de-model.pt")
```

    /home/tenyun/.conda/envs/pytorch3/lib/python3.6/site-packages/torch/serialization.py:325: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.
      warnings.warn(msg, SourceChangeWarning)
    /home/tenyun/.conda/envs/pytorch3/lib/python3.6/site-packages/torch/serialization.py:325: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.
      warnings.warn(msg, SourceChangeWarning)



```python
model.eval()
sent = "â–The â–log â–file â–can â–be â–sent â–secret ly â–with â–email â–or â–FTP â–to â–a â–specified â–receiver".split()
src = torch.LongTensor([[SRC.stoi[w] for w in sent]])
src = Variable(src)
src_mask = (src != SRC.stoi["<blank>"]).unsqueeze(-2)
out = greedy_decode(model, src, src_mask, 
                    max_len=60, start_symbol=TGT.stoi["<s>"])
print("Translation:", end="\t")
trans = "<s> "
for i in range(1, out.size(1)):
    sym = TGT.itos[out[0, i]]
    if sym == "</s>": break
    trans += sym + " "
print(trans)
```

    Translation:	<s> â–Die â–Protokoll datei â–kann â– heimlich â–per â–E - Mail â–oder â–FTP â–an â–einen â–bestimmte n â–EmpfÃ¤nger â–gesendet â–werden . 


### Attentionå¯è§†åŒ–
æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥çœ‹çœ‹ï¼Œçœ‹çœ‹æ¯ä¸€å±‚æ³¨æ„åŠ›å±‚å‘ç”Ÿäº†ä»€ä¹ˆ


```python
tgt_sent = trans.split()
def draw(data, x, y, ax):
    seaborn.heatmap(data, xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, cbar=False, ax=ax)
    
for layer in range(1, 6, 2):
    fig, axs = plt.subplots(1, 4, figsize=(20, 10))
    print("Encoder Layer", layer+1)
    for h in range(4):
        draw(model.encoder.layers[layer].self_attn.attn[0, h].data,
            sent, sent if h==0 else [], ax=axs[h])
    plt.show()
    
for layer in range(1, 6, 2):
    fig, axs = plt.subplots(1, 4, figsize=(20, 10))
    print("Decoder Self Layer", layer + 1)
    for h in range(4):
        draw(model.decoder.layers[layer].self_attn.attn[0, h].data[:len(tgt_sent),:len(tgt_sent)],
            tgt_sent, tgt_sent if h == 0 else [] , ax = axs[h])
    plt.show()
    print("Decoder Src Layer", layer+1)
    fig, axs = plt.subplots(1, 4, figsize=(20, 10))
    for h in range(4):
        draw(model.decoder.layers[layer].self_attn.attn[0, h].data[:len(tgt_sent), :len(sent)],
            sent, tgt_sent if h==0 else [], ax=axs[h])
    plt.show()
```

    Encoder Layer 2



![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_92_1.png)


    Encoder Layer 4



![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_92_3.png)


    Encoder Layer 6



![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_92_5.png)


    Decoder Self Layer 2



![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_92_7.png)


    Decoder Src Layer 2



![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_92_9.png)


    Decoder Self Layer 4



![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_92_11.png)


    Decoder Src Layer 4



![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_92_13.png)


    Decoder Self Layer 6



![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_92_15.png)


    Decoder Src Layer 6



![png](The%20Annotated%20Transformer_files/The%20Annotated%20Transformer_92_17.png)



```python

```


```python

```
